---
title: "TabulaR Playground"
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 5
    fig_height: 4
    theme: cosmo
    highlight: tango
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<center>
![](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Multiplication_table_to_scale.svg/1200px-Multiplication_table_to_scale.svg.png){width=40%}
</center>

# Introduction
This is a starter notebook for the Tabular Playground Series - Jan 2021 competition with R language. The main goal of the kernel is to present steps needed for building a model using [R](https://www.r-project.org/), [tidyverse](https://www.tidyverse.org/), and, possibly, [torch](https://torch.mlverse.org/) and [lightgbm](https://cran.r-project.org/web/packages/lightgbm/index.html) packages.

This is a supervised machine learning problem which is evaluated on [the root mean squared error](https://en.wikipedia.org/wiki/Root-mean-square_deviation):
$$
RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y_i})}
$$

# Preparations {.tabset .tabset-fade}
## Libraries
```{r load_lib, message=FALSE, warning=FALSE, results='hide'}
library(knitr)
library(torch)
library(recipes)
library(rsample)
library(lightgbm)
library(tidyverse)
library(ggcorrplot)
```
## Constants
```{r set_const, message=FALSE, warning=FALSE, results='hide'}
set.seed(0)

path <- "../input/tabular-playground-series-jan-2021/"
kfolds <- 5
```
## Load Data
```{r load_tab, message=FALSE, warning=FALSE, results='hide'}
tr <- read_csv(str_c(path, "train.csv"))
te <- read_csv(str_c(path, "test.csv"))
sub <- read_csv(str_c(path, "sample_submission.csv"))
```

# Tabular Overview {.tabset .tabset-fade}
## Train
```{r tr_csv, message=FALSE, warning=FALSE, echo=TRUE, results='show'}
head(tr, 5) %>% kable()
```
## Test
```{r te_csv, message=FALSE, warning=FALSE, echo=TRUE, results='show'}
head(te, 5) %>% kable()
```
## Submission
```{r sub_csv, message=FALSE, warning=FALSE, echo=TRUE, results='show'}
head(sub, 5) %>% kable()
```
<div></div>

# Train & Test Set
## Glimpse at the dataset
Let's have a closer look at the training set:
```{r tr1, message=FALSE, warning=FALSE, echo=TRUE, results='show'}
glimpse(tr)
```
In total we have 16 columns:

* An **id** column
* 14 continuous feature columns
* A **target** column

The test set has the same columns except for the **target**, which we have to predict.

## Distributions: Train vs Test
```{r tr_fea_dist, message=FALSE, warning=FALSE, echo=TRUE, results='show', fig.height=12, fig.align='center'}
tr %>% 
  select(starts_with("cont")) %>% 
  mutate(grp = "train") %>%
  bind_rows(
    (te %>% 
       select(starts_with("cont")) %>% 
       mutate(grp = "test"))
  ) %>% 
  pivot_longer(cols = starts_with("cont")) %>% 
  group_by(name, grp) %>% 
  mutate(mean = mean(value)) %>% 
  ggplot(aes(x = value)) + 
  facet_wrap(~name, ncol = 2, scales = "free") +
  geom_density(aes(fill = grp), alpha = 0.3) +
  geom_vline(aes(xintercept = mean), linetype = "dashed", size = 0.2) +
  theme_minimal() + 
  theme(legend.position = "top") +
  labs(fill = "")
```

* Both train and test distributions are identical
* We might hope that our model will work correctly with the test set

## Features Correlations
In the plot below we might see that no feature has significant correlation with the **target**. Several features correlate with each other, e.g.,  **cont6** vs **cont10**, **cont11** vs **cont12**. We might try to remove some highly correlated variables.
```{r tr_corr, message=FALSE, warning=FALSE, echo=TRUE, results='show', fig.height=6, fig.align='center'}
tr %>% 
  select(-id) %>% 
  cor() %>% 
  round(1) %>% 
  ggcorrplot(hc.order = TRUE, 
             type = "lower",
             p.mat = cor_pmat(.), 
             insig = "blank",
             colors = c("#6D9EC1", "white", "#E46726"))
```

## Target 
```{r tr_tar1, message=FALSE, warning=FALSE, echo=TRUE, results='show', fig.align='center'}
summary(tr$target)
```

```{r tr_tar2, message=FALSE, warning=FALSE, echo=TRUE, results='show', fig.align='center'}
tr %>% 
  select(target) %>% 
  mutate(log_target = log1p(target)) %>% 
  pivot_longer(cols = everything()) %>% 
  group_by(name) %>% 
  mutate(mean = mean(value)) %>% 
  ggplot(aes(x = value)) +
  facet_wrap(~ name, ncol = 2, scales = "free") +
  geom_density(aes(fill=name), alpha = 0.3) +
  geom_vline(aes(xintercept = mean), linetype = "dashed", size = 0.2) +
  theme_minimal() + 
  theme(legend.position = "none") +
  labs(fill = "", x = "", y = "")
```

* Distribution of the **target** column is bimodal 
* This is a left-skewed distribution
* Log-transform of the **target** column doesn't help a lot

# Data Preprocessing
At this step we use the [recipes](https://cran.r-project.org/web/packages/recipes/index.html) package, which makes data preprocessing much easier. We remove **id** and **target** columns and normalize numeric columns:
```{r pre1, message=FALSE, warning=FALSE, echo=TRUE, results='show', fig.align='center'}
(rec <- tr %>%
   recipe(~ .) %>%
   step_rm(id, target) %>% 
   step_normalize(all_numeric()) %>%
   prep())
```
Let's prepare the dataset using the recipe:
```{r pre2, message=FALSE, warning=FALSE, results='hide'}
y <- tr$target
X <- juice(rec, composition = "matrix")
X_te <- bake(rec, te, composition = "matrix")
```
As a result we get matrices.

# LightGBM
## Training a GBM Model
Here we do a simple training loop over k-folds:
```{r gbm1, message=FALSE, warning=FALSE, results='show'}
p <- list(objective = "regression", 
          metric = "rmse",
          feature_pre_filter = FALSE,
          learning_rate = 0.0045,
          num_leaves = 102,
          min_child_samples = 20,
          sub_feature = 0.4,
          sub_row = 1,
          subsample_freq = 0,
          lambda_l1 = 4.6,
          lambda_l2 = 1.9)

preds <- 0
scores <- c()
imp <- tibble()
for (rs in vfold_cv(tr, kfolds)$splits) {
  tri <- rs$in_id
  
  xtr <- lgb.Dataset(X[tri, ], label = y[tri])
  xval <- lgb.Dataset(X[-tri, ], label = y[-tri])
  
  m_lgb <- lgb.train(params = p,
                     data = xtr,
                     nrounds = 10000,
                     valids = list(val = xval),
                     early_stopping_rounds = 100,
                     eval_freq = 500, 
                     verbose = -1)
  
  scores <- c(scores, m_lgb$best_score)
  cat("Current score for", rs$id$id, ":", m_lgb$best_score, "\n")
  imp <- bind_rows(imp, lgb.importance(m_lgb))
  
  preds <- preds + predict(m_lgb, X_te)
}
```
```{r gbm2, message=FALSE, warning=FALSE, results='show'}
cat("Mean score:", mean(scores), "\n")
```

## Feature Importance in GBM
```{r gbm3, message=FALSE, warning=FALSE, results='show'}
imp %>% 
  group_by(Feature) %>% 
  summarise(Gain = mean(Gain)) %>% 
  ggplot(aes(reorder(Feature, Gain), Gain)) + 
  geom_col(fill = "steelblue") +
  xlab("Feature") +
  ggtitle("GBM") +
  coord_flip() +
  theme_minimal()
```

## GBM Submission
```{r sub1, message=FALSE, warning=FALSE, results='show'}
sub %>% 
  mutate(target = preds / kfolds) %>% 
  write_csv("sub_gbm.csv")
```

